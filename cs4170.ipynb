{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c5872d-7dc5-48b0-b988-80d09649a5df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbdcbeef-b432-486c-b0b7-ca92e3fe87b0",
   "metadata": {},
   "source": [
    "## **Course Description**\n",
    "CS 4170 - Data Mining with Applications in the Life Sciences is a computer science elective offered here at Ohio University that is often taken by undergraduate students in their third or fourth year. If you have ever had any interest in mining data or working with data to solve complex problems, CS 4170 would be the perfect course to take to learn more about it. Students in this course will learn all about the Perl programming language and use Perl to design and develop their own custom software to solve real-world life science problems. Some of the topics students will cover in this course include: processing DNA sequences and protein sequences, restriction maps, data pipelines, and the Entrez programming utilities. CS 4170 is an extremely interesting course where students will learn various data mining techniques to help solve modern problems in the life sciences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359a0e1b-b08f-40af-9391-265e2537c8ba",
   "metadata": {},
   "source": [
    "## **Learning Outcomes**\n",
    "- Students will gain the ability to develop Perl programs that combine third party tools to form customized data analysis pipelines\n",
    "- Students will gain the ability to use the Perl programming language to architect and construct software packages that solve computational biology problems\n",
    "- Students will gain the ability to develop Perl programs that perform processing of biological sequence data\n",
    "- Students will learn basic concepts of database management\n",
    "- Students will learn about features of the Bioperl libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6ac90f-209b-47ec-b59a-1ca92b6e2630",
   "metadata": {},
   "source": [
    "## **What You'll Learn**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9851c43-3f0e-4d06-a4b2-0ca1b7be27d6",
   "metadata": {},
   "source": [
    "### **MapReduce**\n",
    "There are many techniques out there that are used to mine data, but one of the most used techinques right now by individuals and big companies is MapReduce. MapReduce is a programming model implementation that is used for processing large sets of data in parallel. The basic MapReduce algorithms works by dividing up the data set into chunks to be processed on different hardware, and then gathers all of the information up from each process once they are finished to come to some sort of conclusion\n",
    "> - **A MapReduce program is comprised of three seperate steps:**\n",
    ">> 1. **The mapping step**\n",
    ">>> - This is where the data gets filtered and sorted. The results of this step are a collection of (key, value) pairs which represent the mapping of the data we are mining.\n",
    ">> 2. **The shuffle step**\n",
    ">>> - The shuffe step acts as an intermediate state between the map and reduce steps. The one job of the shuffle step is to sort all of the (key, value) pairs so the reducer step gets all identical keys\n",
    ">> 3. **The reducing step**\n",
    ">>> - The reduce step is where we perform a specific summary operation. In the program below, the reducer step counts all of the different values for the same key and then outputs each unique pair.\n",
    "\n",
    "A common usage for the MapReduce technique is counting the number of ocurrences of each unique word in a file. Below is a Python program that parses a .txt file and outputs each unique word in the .txt file and the number of times each uniqued word occured in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8e444b-f2c6-4c1d-b03f-d1c153ef6882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code from https://towardsdatascience.com/5-data-mining-techniques-every-data-scientist-should-know-be06426a4ed9\n",
    "#Import needed libraries\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "#Define mapper\n",
    "def mapper(file):\n",
    "    for line in file.readlines():\n",
    "        line = line.strip()\n",
    "        #Get words\n",
    "        words = line.split()\n",
    "        #Count\n",
    "        count = []\n",
    "        for w in words:\n",
    "            print('%s\\t%s' % (w, 1))\n",
    "            count.append('%s\\t%s' % (w, 1))\n",
    "    return count\n",
    "          \n",
    "#Define reducer\n",
    "def reducer(counts):\n",
    "  current_word = None\n",
    "  current_count = 0\n",
    "  word = None\n",
    "  for line in counts:\n",
    "      line = line.strip()\n",
    "      #The input we got from mapper\n",
    "      word, count = line.split('\\t', 1)\n",
    "      try:\n",
    "          count = int(count)\n",
    "      except ValueError:\n",
    "          continue\n",
    "      # If the word is not None\n",
    "      if current_word == word:\n",
    "          current_count += count\n",
    "      else:\n",
    "          if current_word:\n",
    "              print ('%s\\t%s' % (current_word, current_count))\n",
    "          current_count = count\n",
    "          current_word = word\n",
    "\n",
    "#Use the functions on any txt file\n",
    "file1 = open(\"src/myfile.txt\",\"r+\") \n",
    "mapped_results = mapper(file1)\n",
    "#Pass to the resucer\n",
    "reducer(mapped_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38606699-1960-4e27-b718-7a5c52410af7",
   "metadata": {},
   "source": [
    "### **Frequent Itemset Analysis**\n",
    "Frequent Itemset Analysis is another popular data mining technique utlized in in the industry these days that uses the market-based model approach for analyzing data. The market-basket is a model that is used to describe a common form of a many to many relationhsip. This data model is used to connect two types of data points, items & baskets. Each basket has a set of items -- hence, itemset -- and it is often assumed that the size of the itemset is smaller than the total number of items.\n",
    "> - Frequent itemset analysis can be used to categorize and analyze different kinds of applications, for example, let's assume we have some text documents that we want to min for specific words, we can use:\n",
    ">> - **Related Concepts:** Let items be words, and let baskets be documents. If we want to look for some words that appear in many documents, the sets will be dominated by the most common words in documents, such as stop words or connecting words. We can ignore these words to see the most frequent words in the documents.\n",
    ">> - **Plagiarism:** In this case, the items will be the documents and the baskets will be the sentences within the document. An item is a part of a basket if the sentence is in the document. If we want to detect plagiarism, then we try to look for pairs of items that appear together in several baskets within two different documents. If we find such a pair, then we have 2 documents that share several sentences in common, which means that plagiarism exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be059ffa-5ea8-4506-8779-246a127c59bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed library\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "#Create or import a dataset\n",
    "dataset = [['Veggies', 'Onion', 'Nutmeg', 'Black Beans', 'Veggies', 'Yogurt'],\n",
    "           ['Dill', 'Onion', 'Nutmeg', 'Black Beans', 'Veggies', 'Yogurt'],\n",
    "           ['Veggies', 'Apple', 'Black Beans', 'Eggs'],\n",
    "           ['Eggs', 'Unicorn', 'Corn', 'Black Beans', 'Yogurt'],\n",
    "           ['Corn', 'Garlic', 'Garlic', 'Black Beans', 'Ice cream', 'Veggies']]\n",
    "#Transform into corret formate\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(dataset).transform(dataset)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "#Apply algorithm with 70% min confidence\n",
    "apriori(df, min_support=0.7, use_colnames=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb0e32c-9fd8-409a-8df3-0c73e4692e7b",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "CS 4170 is one of the most interesting computer science electives offered here at Ohio University. If you have ever had any interest working with large data sets, using the Perl programming language, or the solving problems in the life sciences, you should seriously consider taking this course. Being a skilled Perl programmer is a rarity in the job market these days so being able to boast your Perl programming skills and software projects developed in this course could intrigue future employers. Also, one of the best parts about this course in particular is that stduents get to learn how to contribute to the world of life sciences by using software to solve modern day problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
