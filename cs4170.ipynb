{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd814ce9-d98b-43bf-b3f3-a5397e77bf14",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "![Russ College Logo](images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d74798-8391-4d68-8e87-ae4386dae497",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<b>\n",
    "    <p style=\"text-align:center;color:#00694E;font-family:copperplate;font-size:40px\">\n",
    "        CS 4170 - Data Mining with Applications in the Life Sciences\n",
    "    </p>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdcbeef-b432-486c-b0b7-ca92e3fe87b0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## **Course Description**\n",
    "CS 4170 - Data Mining with Applications in the Life Sciences - is a computer science elective offered here at Ohio University that is often taken by undergraduate students in their third or fourth year. If you have ever had any interest in mining data or working with data to solve complex problems, CS 4170 would be the perfect course for you! Students in this course will design and develop their own custom software to solve real-world life science problems. Some of the topics students will cover in this course include: processing DNA sequences and protein sequences, restriction maps, data pipelines, and the Entrez programming utilities. In this class, you will learn about data classification (decision trees and random forests), association rule mining, and data cleaning. CS 4170 is an extremely interesting course where students will learn various data mining techniques to help solve modern problems in the life sciences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359a0e1b-b08f-40af-9391-265e2537c8ba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## **Learning Outcomes**\n",
    "- Students will gain the ability to develop programs that combine third party tools to form customized data analysis pipelines\n",
    "- Students will gain the ability to use the programming language to architect and construct software packages that solve computational biology problems\n",
    "- Students will gain the ability to develop programs that perform processing of biological sequence data\n",
    "- Students will learn basic concepts of database management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6ac90f-209b-47ec-b59a-1ca92b6e2630",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## **What You'll Learn**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dc9e34-e1f9-4ea4-abc6-247ea93ec93e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### **Data Classification**\n",
    "Credit to https://www.geeksforgeeks.org/basic-concept-classification-data-mining/ for the following information:\n",
    "\n",
    "\"Data mining involves mining or \"digging\" deep into data that is in different forms to develop patterns, and to gain knowledge on those patterns. In the process of data mining, large data sets are first sorted, then patterns are identified and relationships are established to perform data analysis and to solve problems.\"\n",
    "<br> <br>\n",
    "\"Data classification is a data analysis task, or a process of finding a model that describes and distinguishes data classes and concepts. Classification is the problem of identifying to which of a set of categories (subpopulations), a new observation belongs to, on the basis of a training set of data containing observations and whose categories membership is known. Two examples of data classification that you will learn more about in this class are decision trees and random forests.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68706b9b-5b55-41fd-aa94-9d08d1532c67",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "#### **Decision Trees**\n",
    "\n",
    "Credit to https://www.softwaretestinghelp.com/decision-tree-algorithm-examples-data-mining/ for the following information:\n",
    "\n",
    "![Russ College Logo](images/cs4170-decisiontree.png)\n",
    "\n",
    "\"A decision tree is a supervised learning algorithm that works for both discrete and continuous variables. It splits the dataset into subsets on the basis of the most significant attribute in the dataset. How the decision tree identifies this attribute and how this splitting is done is decided by the algorithms.\n",
    "\n",
    "The most significant predictor is designated as the root node, splitting is done to form sub-nodes called decision nodes, and the nodes which do not split further are terminal or leaf nodes.\n",
    "\n",
    "In the decision tree, the dataset is divided into homogeneous and non-overlapping regions. It follows a top-down approach as the top region presents all the observations at a single place which splits into two or more branches that further split. This approach is also called a greedy approach as it only considers the current node between the worked on without focusing on the future nodes.\n",
    "\n",
    "The decision tree algorithms will continue running until a stop criteria such as the minimum number of observations etc. is reached.\n",
    "\n",
    "Once a decision tree is built, many nodes may represent outliers or noisy data. Tree pruning method is applied to remove unwanted data. This, in turn, improves the accuracy of the classification model.\n",
    "\n",
    "To find the accuracy of the model, a test set consisting of test tuples and class labels is used. The percentages of the test set tuples are correctly classified by the model to identify the accuracy of the model. If the model is found to be accurate then it is used to classify the data tuples for which the class labels are not known.\"\n",
    "\n",
    "Watch the following video to learn more about decision trees! https://www.youtube.com/watch?v=7VeUPuFGJHk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc699fee-ce2a-4e39-a1b3-7ad97041fb1b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "#### **Random Forests**\n",
    "Watch the following video to learn more about random forests! https://www.youtube.com/watch?v=J4Wdy0Wc_xQ <br>\n",
    "\n",
    "A random forest is a data classification algorithm that is built from many decision trees. In the real world, a single decision tree can sometimes be inaccurate because they are NOT flexible when creating new samples. A random forest algorithm uses bagging (random samples with replacement) and feature randomness when building each individual tree. This process creates an uncorrelated \"random forest\" -  whose prediction is more accurate than that of any individual tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e11006-5caf-4783-a029-5e9518694ec6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "The following examples below use the third party library, \"scikit-learn\", which is a Python machine learning library that provides an implementation of Random Forest for machine learning. In the example below, the function make_classification() creates a synthetic binary classification problem with 1,000 examples and 20 input features. Using a random forest algorithm, the following code evaluates the model using repeated stratified k-fold cross-validation, with three repeats and 10 folds. The code outputs the mean and standard deviation of the accuracy of the model across all repeats and folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6742834-812b-4119-bf15-25953da8f2b1",
   "metadata": {
    "deletable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure kernel is set to Python3\n",
    "# CREDIT to https://machinelearningmastery.com/random-forest-ensemble-in-python/ for example\n",
    "\n",
    "# evaluate random forest algorithm for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=3)\n",
    "# define the model\n",
    "model = RandomForestClassifier()\n",
    "# evaluate the model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77d5cb9-0721-49c3-8b10-18bc1d4d359f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "This Random Forest has an accuracy of 90.5%, which is pretty high!\n",
    "<br>\n",
    "<br>\n",
    "Random forests can also be used for regression problems. Similarly to the last example, the following code evaluates the model using repeated k-fold cross-validation, with three repeats and 10 folds. The code outputs the mean absolute error (MAE) of the model across all repeats and folds. The scikit-learn library makes the MAE negative so that it is maximized instead of minimized. This means that larger negative MAE are better and a perfect model has a MAE of 0. Run the following code to see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aa2bf2-e333-4ad3-a40d-2c0becf023f6",
   "metadata": {
    "deletable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure kernel is set to Python3\n",
    "# CREDIT to https://machinelearningmastery.com/random-forest-ensemble-in-python/ for example\n",
    "\n",
    "# evaluate random forest ensemble for regression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=2)\n",
    "# define the model\n",
    "model = RandomForestRegressor()\n",
    "# evaluate the model\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# report performance\n",
    "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec71d8d5-f225-452a-959e-3da1aec4b149",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Since the Mean Absolute Error statistic is a large negative number, it means that the random forest classified the data well. Learn more the sklearn python library at https://scikit-learn.org/stable/, and feel free to experiment with it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae3590b-59bb-4b5e-adf8-437c4406da20",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### **Data Cleaning**\n",
    "Data cleaning is a very important step in the process of mining and analyzing data. As indicated in its name, data cleaning involves cleaning up the raw data that you mine in order to analyze it. This involves but is not limited to, fixing/removing incorrect data, incomplete data, irrelevant data, and duplicate data. The process of data cleaning can also involve removing \"noise\" from a dataset, which is data that is meaningless and does not contribute to the overall trend of the data.\n",
    "Refer to the following article to learn more about the process of data cleaning! https://www.javatpoint.com/data-cleaning-in-data-mining\n",
    "\n",
    "Credit to https://algodaily.com/lessons/introduction-to-data-cleaning-and-wrangling/why-do-we-clean-data for the following image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc2c6fc-ecb5-4800-8b71-f6d4dcff8ecb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "![Data cleaning](images/cs4170-cleaning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95540c77-32f6-4cf6-8792-0a707884b9f7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Consider the following example below of a dataset. The data has a positive correlation, but has a fairly large range and a lot of variation among each point. This data set contains a lot of \"noise\" or data that is meaningless information when looking at the entire trend as a whole. Run the code below to view the graph of a data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c39f19a-ded0-4afa-af33-3f16cf7e64c8",
   "metadata": {
    "deletable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure kernel is set to Python3\n",
    "# Credit to https://stackoverflow.com/questions/37598986/reducing-noise-on-data for example\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mu, sigma = 0, 500\n",
    "x = np.arange(1, 100, 0.1)  # x axis\n",
    "z = np.random.normal(mu, sigma, len(x))  # noise\n",
    "y = x ** 2 + z  # signal + noise\n",
    "\n",
    "plt.plot(x, y, linewidth = 2, linestyle = \"-\", c = \"b\")  # includes some noise\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ee931b-11ff-4888-8c7b-451ba9aacba4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "This dataset is in need of some data cleaning! If we clean some of the meaningless points away from the graph, it can help us to better analyze the data and to see any general trends. To do this, we are using scipy, a Python third party library, to clean this dataset by filtering out the noise. Run the code below to see the result after we clean the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1b37e1-4d05-477b-a696-76899ea133d1",
   "metadata": {
    "deletable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure kernel is set to Python3\n",
    "# Credit to https://stackoverflow.com/questions/37598986/reducing-noise-on-data for example\n",
    "from scipy.signal import savgol_filter\n",
    "w = savgol_filter(y, 101, 2)\n",
    "plt.plot(x, w, 'b')  # high frequency noise removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ef4f80-fb25-4fc6-8419-6acb6d29430c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**Coding challenge!** - Change the '101' value in the savgol_filter function to 11 and then 501 to see how much noise is filtered out of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb0e32c-9fd8-409a-8df3-0c73e4692e7b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## **Conclusion**\n",
    "CS 4170 is one of the most interesting computer science electives offered here at Ohio University. If you have ever had any interest working with large data sets or solving problems in the life sciences, you should seriously consider taking this course. Topics in this class include data classification, association rule mining, data cleaning, and more! Also, one of the best parts about this course is that students get to learn how to contribute to the world of life sciences by using software to solve modern day problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4c27b7-5720-4d75-b739-583294d66e25",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<b>\n",
    "    <p style=\"text-align:center;color:#00694E;font-family:copperplate;font-size:13px\">\n",
    "        © 2022 GAMA: Gavin Dassatti, Alex Heffner, Matthew Lang, and Aaron Begy. All rights reserved.\n",
    "    </p>\n",
    "</b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
